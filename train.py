import argparse
import os
import yaml
import torch
import numpy as np
from torch.utils.data import DataLoader
from tqdm import tqdm
import torch.optim as optim

# å¯¼å…¥ä½ çš„æ¨¡å—
from dataset import InductionHardeningDataset
from model import ParallelUFNO
from losses import CombinedLoss  # ç¡®ä¿ losses.py å·²ç»æ˜¯æœ€æ–°ç‰ˆ

def load_config(config_path):
    with open(config_path, "r", encoding='utf-8') as f:
        return yaml.safe_load(f)

def train():
    # 1. åŸºç¡€è®¾ç½®
    parser = argparse.ArgumentParser()
    parser.add_argument("--config", type=str, default="config.yaml")
    args = parser.parse_args()

    print(f"ğŸ”„ Loading config from {args.config}...")
    cfg = load_config(args.config)
    
    device = torch.device(cfg["training"]["device"] if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Device: {device}")

    # 2. å‡†å¤‡æ•°æ®
    print("ğŸ“‚ Loading datasets...")
    # è‡ªåŠ¨è·å– time_steps
    time_steps = cfg["data"].get("time_steps", 100)
    
    train_dataset = InductionHardeningDataset(
        data_dir="data/processed/npy_data",
        split="train",
        time_steps=time_steps
    )
    val_dataset = InductionHardeningDataset(
        data_dir="data/processed/npy_data",
        split="val",
        time_steps=time_steps
    )

    train_loader = DataLoader(
        train_dataset,
        batch_size=cfg["training"]["batch_size"],
        shuffle=True,
        num_workers=cfg["data"]["num_workers"],
        pin_memory=(device.type == 'cuda')
    )
    val_loader = DataLoader(
        val_dataset,
        batch_size=cfg["training"]["batch_size"],
        shuffle=False,
        num_workers=cfg["data"]["num_workers"],
        pin_memory=(device.type == 'cuda')
    )

    # 3. åŠ è½½å‡ ä½• Mask
    mask = None
    mask_path = os.path.join("data/processed/npy_data", "geometry_mask.npy")
    if os.path.exists(mask_path):
        print("ğŸ­ Loading geometry mask...")
        mask = torch.from_numpy(np.load(mask_path)).float().to(device)
    
    # 4. æ„å»ºæ¨¡å‹
    print("ğŸ—ï¸ Building model...")
    model = ParallelUFNO(
        n_modes=tuple(cfg["model"]["n_modes"]),
        hidden_channels=cfg["model"]["hidden_channels"],
        in_channels=cfg["model"]["in_channels"],
        out_channels=cfg["model"]["out_channels"],
        n_layers=cfg["model"]["n_layers"]
    ).to(device)

    # 5. å®šä¹‰ Loss å’Œä¼˜åŒ–å™¨
    # âš ï¸ ä¿®å¤ç‚¹ï¼šCombinedLoss åˆå§‹åŒ–ä¸å†æ¥å— gamma å’Œ mask
    criterion = CombinedLoss(
        alpha=cfg["loss"]["alpha"],
        beta=cfg["loss"]["beta"]
    ).to(device)

    optimizer = optim.Adam(
        model.parameters(),
        lr=float(cfg["training"]["learning_rate"]),
        weight_decay=float(cfg["training"]["weight_decay"])
    )
    
    scheduler = optim.lr_scheduler.StepLR(
        optimizer,
        step_size=cfg["training"]["scheduler_step_size"],
        gamma=cfg["training"]["scheduler_gamma"]
    )

    # 6. è®­ç»ƒå¾ªç¯
    best_val_loss = float('inf')
    epochs = cfg["training"]["epochs"]
    save_dir = "outputs/models_weights"
    os.makedirs(save_dir, exist_ok=True)

    print("ğŸ”¥ Starting training...")
    for epoch in range(epochs):
        model.train()
        train_loss = 0.0
        
        # è®­ç»ƒæ­¥
        pbar = tqdm(train_loader, desc=f"Epoch {epoch+1}/{epochs} [Train]")
        for x, y in pbar:
            x, y = x.to(device), y.to(device)
            
            optimizer.zero_grad()
            pred = model(x) # è¾“å‡º Raw Logits
            
            # âš ï¸ ä¿®å¤ç‚¹ï¼šåœ¨è®¡ç®— Loss æ—¶ä¼ å…¥ mask
            loss = criterion(pred, y, mask=mask)
            
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            pbar.set_postfix({'loss': loss.item()})
            
        avg_train_loss = train_loss / len(train_loader)
        
        # éªŒè¯æ­¥
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for x, y in val_loader:
                x, y = x.to(device), y.to(device)
                pred = model(x)
                # åŒæ ·ä¼ å…¥ mask
                loss = criterion(pred, y, mask=mask)
                val_loss += loss.item()
                
        avg_val_loss = val_loss / len(val_loader)
        scheduler.step()

        print(f"Epoch {epoch+1}: Train Loss={avg_train_loss:.6f}, Val Loss={avg_val_loss:.6f}")

        # ä¿å­˜æœ€ä½³æ¨¡å‹
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), os.path.join(save_dir, "best_model.pth"))
            print(f"âœ… Saved best model (Loss: {best_val_loss:.6f})")

    print("ğŸ Training complete!")

if __name__ == "__main__":
    train()