import os
import yaml
import torch
import torch.nn as nn
import torch.optim as optim
from torch.utils.data import DataLoader
from tqdm import tqdm
import numpy as np

# --- å¯¼å…¥æœ¬åœ°æ¨¡å— (ç°åœ¨éƒ½åœ¨æ ¹ç›®å½•ï¼Œéå¸¸æ¸…çˆ½) ---
from model import ParallelUFNO
from dataset import InductionHardeningDataset
from losses import CombinedLoss

def train(config_path="config.yaml"):
    # 1. åŠ è½½é…ç½® (å¼ºåˆ¶ UTF-8ï¼Œè§£å†³ Windows ä¸‹ä¸­æ–‡æ³¨é‡ŠæŠ¥é”™)
    print(f"ğŸ”„ Loading config from {config_path}...")
    with open(config_path, "r", encoding="utf-8") as f:
        cfg = yaml.safe_load(f)

    # 2. è®¾ç½®è®¾å¤‡ (GPU/CPU)
    device = torch.device(cfg["training"]["device"] if torch.cuda.is_available() else "cpu")
    print(f"ğŸš€ Device: {device}")

    # 3. å‡†å¤‡æ•°æ®
    # Windows ä¸‹ num_workers å»ºè®®è®¾ä¸º 0ï¼Œé˜²æ­¢å¤šè¿›ç¨‹æŠ¥é”™
    num_workers = cfg["data"].get("num_workers", 0)
    batch_size = cfg["training"]["batch_size"]

    print("ğŸ“‚ Loading datasets...")
    # è¿™é‡Œå‡è®¾æ•°æ®è¿˜åœ¨åŸæ¥çš„ä½ç½®
    train_dataset = InductionHardeningDataset(data_dir="data/processed/npy_data", split="train")
    val_dataset = InductionHardeningDataset(data_dir="data/processed/npy_data", split="val")

    train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=num_workers)
    val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False, num_workers=num_workers)

    # 4. åŠ è½½å‡ ä½• Mask (å¦‚æœæœ‰)
    # ä½œç”¨ï¼šè®¡ç®— Loss æ—¶åªè®¡ç®—å·¥ä»¶åŒºåŸŸï¼Œå¿½ç•¥èƒŒæ™¯
    mask = None
    mask_path = os.path.join("data/processed/npy_data", "geometry_mask.npy")
    if os.path.exists(mask_path):
        print("ğŸ­ Loading geometry mask...")
        mask = torch.from_numpy(np.load(mask_path)).float().to(device)

    # 5. åˆå§‹åŒ–æ¨¡å‹
    print("ğŸ—ï¸ Building model...")
    model = ParallelUFNO(
        n_modes=tuple(cfg["model"]["n_modes"]),
        hidden_channels=cfg["model"]["hidden_channels"],
        in_channels=cfg["model"]["in_channels"],
        out_channels=cfg["model"]["out_channels"],
        n_layers=cfg["model"]["n_layers"],
        encoder_name=cfg["model"]["encoder_name"],
        encoder_weights=cfg["model"]["encoder_weights"]
    ).to(device)

    # 6. ä¼˜åŒ–å™¨ä¸è°ƒåº¦å™¨
    optimizer = optim.Adam(
        model.parameters(), 
        lr=cfg["training"]["learning_rate"], 
        weight_decay=cfg["training"]["weight_decay"]
    )
    
    scheduler = optim.lr_scheduler.StepLR(
        optimizer, 
        step_size=cfg["training"]["scheduler"]["step_size"], 
        gamma=cfg["training"]["scheduler"]["gamma"]
    )

    # åˆå§‹åŒ–æŸå¤±å‡½æ•° (åˆå¹¶äº† Alpha, Beta, Gamma æƒé‡)
    criterion = CombinedLoss(
        alpha=cfg["loss"]["alpha"],
        beta=cfg["loss"]["beta"],
        gamma=cfg["loss"].get("gamma", 0.0),
        mask=mask
    ).to(device)

    # 7. è®­ç»ƒä¸»å¾ªç¯
    epochs = cfg["training"]["epochs"]
    best_val_loss = float("inf")
    
    # ç¡®ä¿å­˜æ¡£ç›®å½•å­˜åœ¨
    save_dir = "outputs/models_weights"
    os.makedirs(save_dir, exist_ok=True)

    print("ğŸ”¥ Starting training...")
    for epoch in range(1, epochs + 1):
        # --- è®­ç»ƒé˜¶æ®µ ---
        model.train()
        train_loss = 0.0
        # ä½¿ç”¨ tqdm æ˜¾ç¤ºè¿›åº¦æ¡
        pbar = tqdm(train_loader, desc=f"Epoch {epoch}/{epochs} [Train]")
        
        for x, y in pbar:
            x, y = x.to(device), y.to(device)
            
            optimizer.zero_grad()
            pred = model(x)
            loss = criterion(pred, y)
            loss.backward()
            optimizer.step()
            
            train_loss += loss.item()
            # å®æ—¶æ›´æ–°è¿›åº¦æ¡ä¸Šçš„ loss
            pbar.set_postfix({"Loss": f"{loss.item():.4f}"})
        
        avg_train_loss = train_loss / len(train_loader)
        
        # --- éªŒè¯é˜¶æ®µ ---
        model.eval()
        val_loss = 0.0
        with torch.no_grad():
            for x, y in tqdm(val_loader, desc=f"Epoch {epoch}/{epochs} [Val ]"):
                x, y = x.to(device), y.to(device)
                pred = model(x)
                loss = criterion(pred, y)
                val_loss += loss.item()
        
        avg_val_loss = val_loss / len(val_loader)
        
        # --- è®°å½•ä¸ä¿å­˜ ---
        scheduler.step()
        print(f"ğŸ“Š Epoch {epoch} Summary: Train Loss = {avg_train_loss:.6f} | Val Loss = {avg_val_loss:.6f}")

        # ç­–ç•¥ A: æ¯ä¸€è½®éƒ½ä¿å­˜ä¸º 'last_model.pth' (ç”¨äºæ–­ç‚¹ç»­è®­)
        torch.save(model.state_dict(), os.path.join(save_dir, "last_model.pth"))

        # ç­–ç•¥ B: åªæœ‰éªŒè¯é›† Loss åˆ›æ–°ä½æ—¶ï¼Œæ‰ä¿å­˜ 'best_model.pth' (ç”¨äºè¯„ä¼°)
        if avg_val_loss < best_val_loss:
            best_val_loss = avg_val_loss
            torch.save(model.state_dict(), os.path.join(save_dir, "best_model.pth"))
            print(f"ğŸŒŸ New best model saved! (Loss: {best_val_loss:.6f})")
            
    print("âœ… Training complete.")

if __name__ == "__main__":
    train()