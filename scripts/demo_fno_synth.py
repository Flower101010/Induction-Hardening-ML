"""
FNO Demo Script (Synthetic Data)
================================
FNO æ¼”ç¤ºè„šæœ¬ï¼ˆåˆæˆæ•°æ®ï¼‰
================================

This script demonstrates a basic usage of the Fourier Neural Operator (FNO)
æ­¤è„šæœ¬æ¼”ç¤ºäº†å‚…é‡Œå¶ç¥žç»ç®—å­ (FNO) çš„åŸºæœ¬ç”¨æ³•
on a synthetic diffusion dataset. It serves as a proof-of-concept or "smoke test"
åœ¨åˆæˆæ‰©æ•£æ•°æ®é›†ä¸Šã€‚å®ƒä½œä¸ºä¸€ä¸ªæ¦‚å¿µéªŒè¯æˆ–â€œå†’çƒŸæµ‹è¯•â€
to ensure the FNO model and training pipeline are working correctly before
ä»¥ç¡®ä¿ FNO æ¨¡åž‹å’Œè®­ç»ƒæµç¨‹åœ¨åº”ç”¨ä¹‹å‰æ­£å¸¸å·¥ä½œ
applying them to the more complex induction hardening data.
åº”ç”¨åˆ°æ›´å¤æ‚çš„æ„Ÿåº”æ·¬ç«æ•°æ®ä¹‹å‰ã€‚
"""

import torch
from torch import nn
from torch.utils.data import Dataset, DataLoader
from neuralop.models import FNO


class DiffusionToy(Dataset):
    """Synthetic 2D diffusion dataset with smooth fields for FNO smoke tests."""

    """ç”¨äºŽ FNO å†’çƒŸæµ‹è¯•çš„å…·æœ‰å¹³æ»‘åœºçš„åˆæˆ 2D æ‰©æ•£æ•°æ®é›†ã€‚"""

    def __init__(self, n: int = 200, size: int = 64) -> None:
        self.n = n
        self.size = size
        xs = torch.linspace(0.0, 1.0, size)
        ys = torch.linspace(0.0, 1.0, size)
        X, Y = torch.meshgrid(xs, ys, indexing="ij")
        self.coord = torch.stack([X, Y], dim=-1)  # [H, W, 2]

    def __len__(self) -> int:
        return self.n

    def __getitem__(self, idx: int):  # type: ignore[override]
        freq = torch.randint(1, 4, ()).float()
        phase = torch.rand(())
        base = torch.sin(freq * torch.pi * self.coord[..., 0] + phase) * torch.cos(
            freq * torch.pi * self.coord[..., 1] + phase
        )
        T0 = base + 0.05 * torch.randn_like(base)  # noisy initial temperature
        # å™ªå£°åˆå§‹æ¸©åº¦
        T = torch.exp(-0.5 * freq) * base  # simple decay target
        # ç®€å•è¡°å‡ç›®æ ‡
        x = torch.cat([self.coord, T0.unsqueeze(-1)], dim=-1)  # [H, W, 3]
        y = torch.stack([T, torch.zeros_like(T)], dim=-1)  # [H, W, 2]
        return x.float(), y.float()


def get_model(
    n_modes: tuple[int, int] = (16, 16),
    hidden_channels: int = 32,
    in_channels: int = 3,
    out_channels: int = 2,
) -> FNO:
    return FNO(
        n_modes=n_modes,
        hidden_channels=hidden_channels,
        in_channels=in_channels,
        out_channels=out_channels,
    )


def train_step(
    model: nn.Module,
    batch: tuple[torch.Tensor, torch.Tensor],
    optim: torch.optim.Optimizer,
    loss_fn: nn.Module,
    device: torch.device,
) -> float:
    x, y = batch
    x = x.to(device)  # [B, H, W, 3]
    y = y.to(device)  # [B, H, W, 2]
    x = x.permute(0, 3, 1, 2)  # [B, C, H, W]
    y = y.permute(0, 3, 1, 2)
    # å¼ºåˆ¶ç‰©ç†èŒƒå›´
    pred = model(x)
    pred_phase = torch.clamp(pred[:, 1:2], 0.0, 1.0)  # enforce physical range
    pred = torch.cat([pred[:, 0:1], pred_phase], dim=1)
    loss = loss_fn(pred, y)
    optim.zero_grad()
    loss.backward()
    optim.step()
    return float(loss.item())


def main() -> None:
    print("ðŸš€ Starting FNO minimal example...")

    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    print(f"   Device: {device}")

    print("ðŸ“¦ Generating synthetic data (DiffusionToy)...")
    ds = DiffusionToy(n=256, size=64)
    dl = DataLoader(ds, batch_size=8, shuffle=True)

    print("ðŸ§  Initializing FNO model...")
    model = get_model().to(device)

    optim = torch.optim.Adam(model.parameters(), lr=1e-3)
    loss_fn = nn.MSELoss()

    print("ðŸ”„ Starting training loop (3 epochs)...")
    initial_loss = None
    final_loss = None

    for epoch in range(3):
        losses = [train_step(model, batch, optim, loss_fn, device) for batch in dl]
        avg_loss = sum(losses) / len(losses)
        if epoch == 0:
            initial_loss = avg_loss
        final_loss = avg_loss
        print(f"   Epoch {epoch + 1}/3: loss={avg_loss:.4f}")

    print("\nâœ… Training finished!")
    if initial_loss is not None and final_loss is not None:
        print(f"   Initial Loss: {initial_loss:.4f}")
        print(f"   Final Loss:   {final_loss:.4f}")

        if final_loss < initial_loss:
            print("ðŸŽ‰ Success: Model is learning (loss decreased).")
        else:
            print("âš ï¸ Warning: Loss did not decrease. Check hyperparameters or data.")


if __name__ == "__main__":
    main()
