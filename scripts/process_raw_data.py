import pandas as pd
import re
import os

# ================= ç”¨æˆ·é…ç½®åŒº =================
# è¾“å…¥æ–‡ä»¶è·¯å¾„ (è¯·ä¿®æ”¹ä¸ºä½ å®é™…çš„æ–‡ä»¶å)
INPUT_FILE = "data/raw/training_data.csv"

# è¾“å‡ºæ–‡ä»¶å¤¹è·¯å¾„ (è„šæœ¬ä¼šè‡ªåŠ¨åˆ›å»º)
OUTPUT_DIR = "data/processed_parquet"

# æ¯æ¬¡å¤„ç†çš„è¡Œæ•° (2000è¡Œæ˜¯ä¸€ä¸ªåœ¨é€Ÿåº¦å’Œå†…å­˜ä¹‹é—´å¾ˆå¥½çš„å¹³è¡¡ç‚¹)
CHUNK_SIZE = 2000
# ============================================


def extract_metadata_from_header(header_line):
    """
    è§£æ COMSOL å¤æ‚çš„è¡¨å¤´ï¼Œç”Ÿæˆåˆ—ååˆ°å‚æ•°çš„æ˜ å°„å­—å…¸ã€‚
    """
    # å»æ‰å¼€å¤´çš„ %ï¼ŒæŒ‰é€—å·åˆ†å‰²ï¼Œå¹¶å»é™¤é¦–å°¾ç©ºæ ¼
    raw_cols = [c.strip() for c in header_line.replace("%", "").strip().split(",")]

    meta_map = {}

    # å®šä¹‰åæ ‡åˆ— (é€šå¸¸å‰ä¸¤åˆ—æ˜¯åæ ‡ï¼Œæ ¹æ®ä½ çš„æ•°æ®è°ƒæ•´)
    coord_cols = ["r", "z"]

    print(f"æ­£åœ¨è§£æ {len(raw_cols)} ä¸ªåˆ—åä¿¡æ¯ï¼Œè¯·ç¨å€™...")

    for col in raw_cols:
        if col in coord_cols:
            continue

        # --- æ­£åˆ™è¡¨è¾¾å¼æå–æ ¸å¿ƒé€»è¾‘ ---
        # 1. æå–ç‰©ç†é‡åç§° (æˆªå– @ ç¬¦å·å‰é¢çš„éƒ¨åˆ†)
        # ä¾‹å¦‚ "T (degC) @ ..." -> "T"
        if "@" in col:
            phys_name_part = col.split("@")[0].strip()
        else:
            # åº”å¯¹æŸäº›æ²¡å†™ @ çš„å¼‚å¸¸æƒ…å†µï¼Œç›´æ¥ç”¨æ•´ä¸ªåˆ—å
            phys_name_part = col

        # å»æ‰æ‹¬å·å†…çš„å•ä½ï¼Œä¾‹å¦‚ "T (degC)" -> "T"
        phys_name = re.sub(r"\s*.âˆ—?.*?.âˆ—?", "", phys_name_part).strip()

        # 2. æå–å‚æ•°æ•°å€¼ (æ”¯æŒæ•´æ•°ã€å°æ•°ã€ç§‘å­¦è®¡æ•°æ³•)
        # æŸ¥æ‰¾ t=..., f_set=..., I_factor=...
        t_match = re.search(r"t=\s*([-+]?[\d\.eE]+)", col)
        f_match = re.search(r"f_set=\s*([-+]?[\d\.eE]+)", col)
        i_match = re.search(r"I_factor=\s*([-+]?[\d\.eE]+)", col)

        meta_map[col] = {
            "variable": phys_name,  # ç‰©ç†é‡åç§° (T, audc.phase5.xi ç­‰)
            "t": float(t_match.group(1)) if t_match else 0.0,
            "f": float(f_match.group(1)) if f_match else 0.0,
            "I": float(i_match.group(1)) if i_match else 0.0,
        }

    return raw_cols, coord_cols, meta_map


def process_big_csv():
    # åˆ›å»ºè¾“å‡ºç›®å½•
    if not os.path.exists(OUTPUT_DIR):
        os.makedirs(OUTPUT_DIR)
        print(f"åˆ›å»ºè¾“å‡ºç›®å½•: {OUTPUT_DIR}")

    # --- ç¬¬ä¸€æ­¥ï¼šç¨³å¥åœ°æŸ¥æ‰¾å¹¶è¯»å–è¡¨å¤´ ---
    print("Step 1: æ‰«ææ–‡ä»¶è¡¨å¤´...")

    last_comment = None
    header_row_index = -1

    try:
        with open(INPUT_FILE, "r", encoding="utf-8") as f:
            # æ‰«æå‰ 100 è¡Œå¯»æ‰¾è¡¨å¤´
            for i in range(100):
                line = f.readline()
                if not line:
                    break

                # è®°å½•æœ€åä¸€è¡Œä»¥ % å¼€å¤´çš„è¡Œ
                if line.strip().startswith("%"):
                    last_comment = line
                    header_row_index = i

                # å¦‚æœé‡åˆ°éæ³¨é‡Šä¸”éç©ºçš„è¡Œï¼Œè¯´æ˜æ³¨é‡ŠåŒºç»“æŸ
                if not line.strip().startswith("%") and line.strip():
                    break
    except FileNotFoundError:
        print(f"âŒ é”™è¯¯: æ‰¾ä¸åˆ°æ–‡ä»¶ '{INPUT_FILE}'")
        return

    # æ£€æŸ¥æ˜¯å¦æ‰¾åˆ°è¡¨å¤´
    if last_comment is None:
        print("âŒ é”™è¯¯: æœªæ‰¾åˆ°ä»¥ '%' å¼€å¤´çš„è¡¨å¤´è¡Œã€‚è¯·æ£€æŸ¥ CSV æ ¼å¼ã€‚")
        return

    print(f"âœ… æ‰¾åˆ°è¡¨å¤´ (ä½äºç¬¬ {header_row_index + 1} è¡Œ)")

    # è§£æè¡¨å¤´å…ƒæ•°æ®
    raw_cols, coord_cols, meta_map = extract_metadata_from_header(last_comment)
    print(f"âœ… è§£æå®Œæˆï¼å°†å¤„ç† {len(meta_map)} ä¸ªæ•°æ®å˜é‡åˆ—ã€‚")

    # --- ç¬¬äºŒæ­¥ï¼šåˆ†å—è¯»å–å¹¶æ¸…æ´— ---
    print(f"\nStep 2: å¼€å§‹åˆ†å—è½¬æ¢ (Chunk Size: {CHUNK_SIZE})...")

    # ä½¿ç”¨ Pandas è¯»å–ï¼Œè·³è¿‡å‰é¢çš„æ³¨é‡Šè¡Œ
    # æ³¨æ„ï¼šnames=raw_cols å¼ºåˆ¶æŒ‡å®šåˆ—åï¼Œé¿å… pandas å†æ¬¡å»è¯»è¡¨å¤´
    reader = pd.read_csv(
        INPUT_FILE, comment="%", header=None, names=raw_cols, chunksize=CHUNK_SIZE
    )

    chunk_id = 0
    total_rows_processed = 0

    for chunk in reader:
        print(f"   >>> æ­£åœ¨å¤„ç†ç¬¬ {chunk_id + 1} å—...", end="\r")

        # 1. å®½è¡¨è½¬é•¿è¡¨ (Melt)
        # å°† [r, z, T@t1, T@t2...] è½¬æ¢ä¸º [r, z, åŸåˆ—å, æ•°å€¼]
        melted = pd.melt(
            chunk, id_vars=coord_cols, var_name="original_col", value_name="value"
        )

        # 2. æ˜ å°„å…ƒæ•°æ®
        # å°† original_col æ›¿æ¢ä¸ºå…·ä½“çš„ t, f, I, variable
        # ä¸ºäº†æ€§èƒ½ï¼Œå…ˆå°† meta_map è½¬æ¢ä¸º DataFrame è¿›è¡Œ Merge
        meta_df = pd.DataFrame.from_dict(meta_map, orient="index")
        meta_df.index.name = "original_col"

        # åˆå¹¶æ•°æ®
        processed_chunk = melted.merge(meta_df, on="original_col", how="left")

        # 3. æ¸…ç†ä¸éœ€è¦çš„åˆ—
        processed_chunk.drop(columns=["original_col"], inplace=True)

        # 4. æ•°æ®ç±»å‹ä¼˜åŒ– (Float64 -> Float32)
        # è¿™å¯¹äº ML è‡³å…³é‡è¦ï¼Œèƒ½èŠ‚çœ 50% å†…å­˜
        cols_to_float32 = ["t", "f", "I", "value"] + coord_cols
        for col in cols_to_float32:
            if col in processed_chunk.columns:
                processed_chunk[col] = processed_chunk[col].astype("float32")

        # 5. ä¿å­˜ä¸º Parquet å°æ–‡ä»¶
        save_name = f"{OUTPUT_DIR}/part_{chunk_id:04d}.parquet"
        processed_chunk.to_parquet(save_name, index=False)

        total_rows_processed += len(chunk)
        chunk_id += 1

    print("\n\nâœ… å…¨éƒ¨å®Œæˆï¼")
    print(f"ğŸ“ æ¸…æ´—åçš„æ•°æ®å·²ä¿å­˜åœ¨: {OUTPUT_DIR}/")
    print(
        f"ğŸ§  æ¥ä¸‹æ¥çš„å»ºè®®: ä½¿ç”¨ pd.read_parquet('{OUTPUT_DIR}') å³å¯è¯»å–æ•´ä¸ªæ•°æ®é›†ç”¨äºè®­ç»ƒã€‚"
    )


if __name__ == "__main__":
    process_big_csv()
